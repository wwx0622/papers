# 标题
[Domain-Adversarial Training of Neural Networks](https://arxiv.org/abs/1505.07818)

# 摘要
提出了一种基于对抗的领域适应方法。
- 场景：源域数据带有标记，目标鱼没有标记。
  1. 源域上任务可分
  2. 域之间的偏移不可分
- 使用梯度反转层(gradient reverse layer)实现。

# 结论
本文提出了一种新的前馈神经网络域自适应方法，该方法允许基于源域的大量带标注数据和目标域的大量未标注数据进行大规模训练。

- 自适应是通过调整两个域中的特征分布来实现的。
- 对齐是通过标准的反向传播训练完成的。

DANN：网络隐藏层学习一种表征，这种表征可以预测源示例标签，但对输入域（源或目标）没有任何信息。
我们在浅层和深层前馈架构中实现了这种新方法。后者通过引入一个简单的梯度反转层，允许在几乎任何深度学习包中简单地实现。
我们的方法的一个方便方面是，域自适应组件可以添加到几乎任何可以通过反向传播进行训练的神经网络结构中。

# 模型
![网络结构](/image/DANN001.png "所提出的体系结构包括一个深度特征提取器（绿色）和一个深度标签预测器（蓝色），它们共同构成一个标准的前馈体系结构。
无监督域自适应是通过添加一个域分类器（red）来实现的，该分类器通过梯度反转层连接到特征提取器，梯度反转层在基于反向传播的训练期间将梯度乘以某个负常数。否则，训练将标准化进行，并将标签预测损失（对于源示例）和域分类损失（对于所有样本）降至最低。梯度反转确保两个域上的特征分布相似（对于域分类器来说尽可能不可区分），从而产生域不变特征。")
## 网络结构
隐藏层：输入m维向量，输出D维表示$G_f(x;W,b)=sigm(Wx+b)$
其中：$sigm(a)=[\frac{1}{1+e^{-a}}]_{i=1}^{|a|}$
最后通过softmax进行L分类：$G_y:R^D\to [0, 1]^L$
对数可能性作为损失函数：$L_y(G_y(G_f(x_i)), y_i)=log\frac{1}{G_y(G_f(x))_{y_i}}$
源域和目标域样本可以分别写作：
- $S(G_y)=\{G_y(x)|x\in S\}$
- $T(G_y)=\{G_y(x)|x\in T\}$

根据前面的原理：源域和目标域可以写作：
$\hat{d_H}(S(G_f), T(G_f))=2(1-min_{\eta \in H}[\frac{1}{n}\sum_{i=1}^{n}I[\eta(G_f(x_i))=0]+\frac{1}{n\prime}\sum_{i=n+1}^{N}I[\eta(G_f(x_i))=1]])$其中的$\eta$是用来计算源域和目标域的分类器的，$I$是一个常数，$n$和$n\prime$是源域和目标域的样本数。
域分类器的损失为：$G_d(G_f(x);u,z)=sigm(u^TG_f(x)+z)$
对于标签预测器来说，当其表现较差时，转移到与域分类器进行分类，所以正则化项可以设置为：
$R(W,b)=max_{u,z}-[\frac{1}{n}\sum_{i=1}{n}L_d^i(W,b,u,z)+\frac{1}{n\prime}\sum_{i=n+1}{N}L_d^i(W,b,u,z)]$
如此可以得到最终目标函数：
$E(W,V,b,c,u,z)=\frac{1}{n}\sum_{i=1}{n}L_y^i(W,b,V,c)-\lambda(\frac{1}{n}\sum_{i=1}{n}L_d^i(W,b,u,z)+\frac{1}{n\prime}\sum_{i=n+1}{N}L_d^i(W,b,u,z))$