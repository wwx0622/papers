# title
[Learning high-order structural and attribute information by knowledge graph attention networks for enhancing knowledge graph embedding](https://www.sciencedirect.com/science/article/pii/S0950705122004853)
利用知识图注意网络学习高阶结构和属性信息以增强知识图嵌入

# Abstract
知识图表示学习的目标是将实体和关系编码到低维嵌入空间中。
当前问题：
1. 现有方法简单地考虑了实体之间的直接关系，无法表达实体之间的高阶结构关系
2. 这些方法简单地利用了知识图的关系三元组，而忽略了编码丰富语义信息的大量属性三元组

为了克服这些限制，本文提出了一种新的知识图嵌入方法（**KANE**），其灵感来自于图卷积网络（GCN）的最新发展。KANE可以在图卷积网络框架下以高效、明确和统一的方式捕获知识图的高阶结构和属性信息。

# Conclusion
- 首先，TransE及其大多数扩展只考虑实体之间的直接关系。
- 其次，大多数现有的知识图嵌入方法只是利用KG的关系三元组，而忽略了大量属性三元组。

KANE的核心思想是在计算给定实体的表示时，将所有属性三元组进行有偏差的聚合，并基于关系三元组执行嵌入传播。在三个数据集上得到了最好的结果。

在未来的研究中，该团队将在几个方面改进现有方法：
1. 首先，未来研究的一个潜在方向是考虑属性嵌入层中**不同属性信息的权重**。
2. 希望通过在注意力嵌入传播阶段替代TransE，重点探索许多其他基于翻译的方法的有效性。
3. 在未来的研究中，应该探索更多类型的嵌入聚合阶段的**聚合器**。

# Introduction
知识图已成为许多人工智能应用的重要基础，如推荐系统、问答和信息检索。在这些人工智能应用中应用KG的一种常见方法是通过嵌入，它提供了一种简单的方法，将实体和关系编码到连续的低维嵌入空间中。
TransE及其大多数扩展只考虑实体之间的直接关系。本文认为，实体之间的**高阶结构关系也包含丰富的语义关系**，合并这些信息可以提高模型性能。
大多数现有的知识图嵌入方法利用KG的**关系三元组**，而忽略了大量**属性三元组**。因此，这些方法容易受到知识图稀疏性和不完全性的影响。更糟糕的是，结构信息通常**无法区分不同三元组中关系和实体的不同含义**。
受GCN的最新发展的启发，提出了用于增强知识图嵌入（**Knowledge Graph Attention Networks fro Enhancing KG Embedding**，KANE）的知识图注意网络，关键思想是，在计算给定实体的表示时，使用**偏差聚合**所有属性三元组，并基于关系三元组执行**嵌入传播**。具体而言，KANE中配备了两个精心设计，以相应地解决以下两个挑战：
1. 基于关系三元组的递归嵌入传播，更新实体嵌入。通过递归地执行这种嵌入传播，可以在线性时间复杂度下成功捕获KG的高阶结构信息；
2. 基于多头注意的聚合。可以通过应用神经注意机制来学习每个属性三元组的权重。

实验在两个KGs任务上评估了该模型，包括知识图完成和实体分类。在三个数据集上的实验结果表明，可以显著优于现有的方法。
本研究的主要贡献如下：
1. 强调了显式建模KG的高阶结构和属性信息以提供更好的知识图嵌入的重要性。
2. 提出了一种新的方法KANE，它可以在图卷积网络框架下以高效、显式和统一的方式捕获KGs的高阶结构和属性信息。
3. 在三个数据集上进行了实验，证明了凯恩在理解高阶关系重要性方面的有效性和可解释性。

# Method
## 整体框架
![整体框架](../../image/KANE%2001.png)
KANE的过程上图所示。
从左到右：
- 知识图的整个三元组作为输入。
- 属性嵌入层的任务是将属性三元组中的每个值嵌入到连续向量空间中，同时保留语义信息。
- 为了捕获KG的高阶结构信息，使用了基于注意的嵌入传播方法。该方法可以递归地从实体的邻居传播实体的嵌入，并聚合具有不同权重的邻居。
- 实体、关系和值的最终嵌入被馈送到两个不同的深度神经网络中，用于两个不同任务，包括链接预测和实体分类。

## 属性嵌入层
属性三元组中的值通常是句子或单词。为了对其句子或单词的值表示进行编码，我们需要将**可变长度的句子编码为固定长度的向量**。在本研究中，采用两种不同的编码器对属性值进行建模。
**Bag-of-Words Encoder**:属性值的表示可以通过单词嵌入中所有值的总和生成。将属性值$a$表示为单词序列$a=w_1，…，w_n$，其中w_i是位置i处的单词。$a$的嵌入可定义为: $\mathbf{a}=\sum_{i=1}^{n}\mathbf{w}_{i}$，$\mathbf{w_i}$表示$w_i$的嵌入向量。
**LSTM Encoder**:顾名思义，通过LSTM网络对属性进行编码。最后的嵌入向量定义为:$\mathbf{a}=f_{l s t m}(\mathbf{w}_{1},\mathbf{w}_{2},\mathbf{w}_{3},\cdot\cdot\cdot,\mathbf{w}_{n})$
<font color='red'>
启发：
对属性进行编码可以用NLP的模型，如Transformer、词袋模型等。以及其他RNN模型：如GRU、Bi-LSTM等。
</font>
## 嵌入传播层
下面介绍基于图卷积网络结构的递归嵌入传播方法的细节。此外，通过利用图注意力网络的思想，KANE学习为实体的每个邻居分配不同级别的重要性，并可以生成级联嵌入传播的注意力权重。在本研究中，嵌入传播层主要由两部分组成：注意嵌入传播和嵌入聚合。
**Attentive Embedding Propagation**:
求h节点的嵌入：h的领域为$\mathcal{N}_{h}$
$$\vec{\mathbf{h}}=\sum_{t\in\mathcal{N}_{h}}\mathcal{\pi}(h,\,r,\,t)\mathbf{W}(\mathbf{r}+\mathbf{t})$$其中的$\pi(h,\,r\,,\,t)$表示头尾节点间的注意力系数。
注意力系数还控制了通过关系从其邻域传播的信息量。为了使注意力系数易于在不同实体之间进行比较，可以在与h相连的所有三元组上使用softmax函数计算注意力系数。softmax函数可以公式化如下：$$\pi(h,\,r,\,t)\Longrightarrow\,\frac{e x p(\pi(h,\,r,\,t))}{\sum_{t^{\prime}{\in}N_{h}}\,e x p(\pi(h,\,r^{\prime},\,t^{\prime}))}$$
此后，我们通过单层前馈神经网络实现注意系数，其公式如下：$$\pi(h, r, t)=\text { LeakyRelu }\left((\mathbf{W r})^{T} \mathbf{W}(\mathbf{r}+\mathbf{t})\right)$$其中的$LeakyRelu()$是激活函数。
<font color = 'red'>这里用$r-t$是不是比较好？</font>
**Embedding Aggregation**:为了稳定注意力的学习过程，在最后一层执行多头注意力。具体而言，使用m注意机制来执行等式的转换。需要一个聚合器来组合多头图注意层的所有嵌入。本研究采用了两种类型的聚合器：
- 连接聚合器连接多头图注意力的所有嵌入，然后进行非线性变换：$$\overrightarrow{\mathbf{h}}^{\prime}=\operatorname{LeakyReLu}\left(\mathbf{W}\left(d \|_{i=1}^{m} \sum_{t \in \mathcal{N}_{h}} \pi(h, r, t)^{i} \mathbf{W}^{i}(\mathbf{r}+\mathbf{t})\right)\right)$$其中||表示串联，$\pi^i$表示由第i个注意嵌入传播计算的归一化注意系数,$\mathbf{W}^i$表示输入嵌入的线性变换。
- 平均聚合器对多头图的所有嵌入求和，并应用平均值计算输出最终嵌入：$$\overrightarrow{\mathbf{h}}^{\prime}=\operatorname{LeakyReLu}\left(\frac{1}{m}\left(\sum_{i=1}^{m} \sum_{t \in \mathcal{N}_{h}} \pi(h, r, t)^{i} \mathbf{W}^{i}(\mathbf{r}+\mathbf{t})\right)\right)$$

为了在KGs中编码高阶连通性信息，使用多个嵌入传播层来收集从其邻居传播的深层信息。更正式地说，实体h在l-th层中的嵌入可以定义如下：$$\overrightarrow{\mathbf{h}}^{(l)}=\sum_{t \in \mathcal{N}_{h}} \pi(h, r, t) \mathbf{W}\left(\mathbf{r}^{(l-1)}+\mathbf{t}^{(l-1)}\right)$$
由此经过L层的传播后得到了最终的嵌入向量。
## 输出层与训练细节
接下来将介绍KANE的学习和优化细节。为KG的两个不同任务精心设计了两个不同的损失函数，包括知识图完成和实体分类。
**KG completion(下面用KGC指代)**:此任务是知识图表示学习社区中的经典任务。具体而言，KGC包括两个子任务：实体预测和链接预测。实体预测的目的是当其中一个实体缺失时，推断测试数据集中不可能的头/尾实体，而链接预测的重点是在关系缺失时完成三元组。本研究借用了TransE的平移评分函数的思想，即尾部实体的嵌入t应接近头部实体的嵌入h加上关系向量r:$$\mathcal{L}=\sum_{(h,r,e)\in{\cal T}}\sum_{(h^{\prime},r,e^{\prime})\in{\cal T}^{\prime}}[\gamma+d(h+r,e)-d(h^{\prime}+r-e^{\prime})]_{+},$$其中，γ>0是边际超参数，$[x]_+$表示x的正部分，T和T'分别表示正(positive)三元组和负三元组，负三元组也即随机挑选头尾实体使得三元组不在知识图中。
**Entity Classification**:对于实体分类任务，只需在最后一层的输出上使用全连接层和sigmoid激活上的二进制交叉熵损失（BCE）。我们最小化所有标记实体上的二进制交叉熵，形式为：$${\mathcal{L}}=-{\frac{1}{|E_{D}|}}\sum_{e\in E_{D}}\sum_{j=1}^{C}[y_{e j}\log(\sigma(f_{e j}))+(1-y_{e j})\log(1-\sigma(f_{e j}))]$$其中$E_D$是有标签的实体集合，C是输出特征的维度，$y_{ej}$是实体$e$的第$j$个标签，激活函数也即sigmoid函数.

使用小批量随机梯度下降（SGD）优化这两个损失函数，并应用chin规则更新所有参数。在每一步中，我新参数$\mathbf{h}^{\tau+1}\leftarrow\mathbf{h}^{\tau}-\lambda\bigtriangledown_{\mathbf{h}}\mathcal{L}$其中τ是迭代步长，λ是学习速率。