# title

[NSCaching: Simple and Efficient Negative Sampling for Knowledge Graph Embedding](https://arxiv.org/abs/1812.06410)
《NSCaching: 针对知识图谱嵌入的简单且高效的负采样方法》

# Abstract

负采样是KG嵌入的一个重要步骤，它从训练数据中未观察到的三联体中抽取负三联体。
最近，在负采样中引入了生成性对抗网络（GAN）。这些方法通过对分数较大的负三元组进行采样，避免了梯度消失的问题，从而获得了更好的性能。
然而，使用GAN会使原始模型更加复杂，难以训练，其中必须使用强化学习。
在本文中，由于观察到分数大的负三联体很重要但很少见，因此我们建议使用缓存(cache)直接跟踪它们。然而，如何从缓存中采样和更新缓存是两个重要的问题。我们精心设计了解决方案，不仅效率高，而且在勘探和开采之间取得了良好的平衡。通过这种方式，我们的方法充当了以前基于GAN的方法的“蒸馏”版本，它不会浪费额外参数的训练时间来拟合负三联体的完整分布。

# Conclusion

提出了NSCaching作为一种新的知识图嵌入学习的负采样方法。负样本来自缓存，该缓存可以动态保存高质量的负样本。我们通过探索与开发的平衡来分析NSCaching的设计。实验上，我们在两个数据集和五个评分函数上对NSCaching进行了实证测试。结果表明，该方法在各种设置下都能很好地泛化，在FB15K数据集上达到了最先进的性能。当处理数百万KG时，存储缓存的内存成为一个问题。未来将继续使用分布式计算或散列。此外，NSCaching的理论收敛也是一项重要而有趣的未来工作。

# Introduction

负采样的需要是因为KG中只有正三联体。为了避免琐碎的嵌入解决方案，对于每个正三联体，需要手工制作一组包含所有可能的负样本的集合。
然后，为了在KG嵌入中随机更新的有效性和效率，一旦我们拾取了一个正三元组，我们还需要从其对应的负样本集中抽取一个负三元组。不幸的是，这些负三元组的质量确实很重要。

由于其简单和高效，均匀采样在KG嵌入中被广泛使用。然而它受到消失梯度问题的严重影响。TransH中介绍了一种更好的采样方案，即贝努利采样。它通过考虑头部和尾部之间的一对多、多对多和多对一映射来改进均匀采样。然而，它仍然是一种固定的采样方案，存在梯度消失问题。

最近，有两项开创性的工作，即IGAN和KBGAN，试图应对这些挑战。他们的想法都是用生成性对抗网络（GAN）取代固定采样方案。然而，基于GAN的解决方案仍然存在许多问题。首先，由于引入了额外的生成器，GAN增加了训练参数的数量。其次，GAN训练可能会出现不稳定性和退化，IGAN和KBGAN中使用的增强梯度已知具有高方差。这些缺点导致不同评分函数的性能不稳定，因此，预处理对于IGAN和KBGAN都是必须的。

在本文中，为了解决高质量负采样的挑战，同时避免使用GAN带来的问题，我们提出了一种新的基于缓存的负采样方法，称为**NSCaching**。通过实证研究负样本的分数分布，我们发现分数分布高度倾斜，即只有少数分数较大的负三联体，其余都是无用的。这一观察结果促使在训练期间只保持高质量的负三胞胎，并动态更新保持的三胞胎。首先，我们将高质量的负三元组存储在缓存中，然后设计重要性抽样（IS）策略来更新缓存。IS策略不仅可以捕获分布的动态特性，而且有利于提高NSCaching的效率。此外，我们还很注意“探索和开发”，它平衡了探索所有可能的高质量负三联体和从缓存中的几个大分数负三联组中取样。我们的工作贡献总结如下：
1. 我们提出了一种简单有效的负采样方案NSCaching。它是一种通用的负采样方案，可以注入到所有常用的KG嵌入模型中。NSCaching的参数比IGAN和KBGAN都少，可以用梯度下降作为原始KG嵌入模型进行训练。
2. 我们提出了从缓存中采样的统一策略和在NSCaching中更新缓存的IS策略，并精心“探索和利用”。
3. 我们分析了NSCaching与自定进度学习之间的关系。我们表明，NSCaching可以首先学习容易分类的样本，然后逐渐切换到较难分类的样本。
4. 我们对四个流行的数据集，即WN18和FB15K，以及它们的变体WN18RR和FB15K237进行了实验。实验结果表明，我们的方法非常有效，并且比现有的方法（即IGAN和KBGAN）更有效。

# Preliminary: Framework of KG Embedding

## A The General Framework
- 基于距离的: 
$$
L(\mathcal{E}, \mathcal{R})=\sum_{(h, r, t) \in \mathcal{S}}[\gamma-f(h, r, t)+f(\bar{h}, r, \bar{t})]_{+}
$$
- 基于语义的: 
$$
L(\mathcal{E}, \mathcal{R})=\sum_{(h, r, t) \in \mathcal{S}}[\ell(+1, f(h, r, t))+\ell(-1, f(\bar{h}, r, \bar{t}))]
$$

![KG嵌入通用框架](../../image/NSCaching%2001.png)

## B. Negative Sampling
现有的负抽样研究可分为两类，即固定分布抽样和动态分布抽样。

1. ***来自固定分布的样本***：即均匀抽样。这种策略简单而有效。随后，在TransH中引入了一种更好的采样方案，即贝努利采样。它通过减少头部和尾部实体之间的一对多、多对多和多对一关系中存在的假阴性三元组的出现来改进均匀采样。
这既不能模拟负三联体分布的动态变化，也不能抽样得分较高的三联体。因此，它们严重受到消失梯度的影响。
2. ***动态分布样本***: 最近, IGAN和KBGAN对固定抽样方案的问题进行了更专门的分析。受GAN的成功及其动态分布建模能力的激励，IGAN和KBGAN引入GAN用于KG中的负采样。

当GAN应用于负采样时，一个联合训练的生成器充当了一个采样器，它不仅可以通过混淆鉴别器生成高质量的三元组，还可以通过保持训练动态适应新的分布。鉴别器，即KG嵌入模型，学习区分生成器选择的正三元组和负三元组。在交替训练过程中，生成器动态逼近负样本分布，并通过高质量的负样本改进KG嵌入模型。

尽管GAN提供了建模动态负样本分布的解决方案，但它以不稳定性和简并性而闻名。此外，必须使用REINFORCE梯度，这是众所周知的高方差。因此，对IGAN和KBGAN来说，预处理都是必须的。
最后，它增加了模型参数的数量，并带来了额外的培训成本。

## C. Scoring Functions
1. ***Translational distance model***: 以距离度量作为评分函数，例如在TransE中，$f(h, r, t) = ||h+r-t||_1$
2. ***Semantic matching model***: 语义匹配, 例如RESCAL的$f(h,r,t)=h^TM_rt$

# Proposed Model

## A. Closer Look at Distribution of Negative Triples

![Distribution of Negative Triples](../../image/NSCaching%2002.png)

## B. NSCaching: the Proposed Method
![NSCaching](../../image/NSCaching%2003.png)
![NSCaching](../../image/NSCaching%2004.png)

1. ***Uniform sampling strategy from the cache***：由于分数越大，梯度越大，一个非常自然的方案是总是对分数最大的负三联体进行采样。然而，由于分布可能在算法迭代期间发生变化，缓存中的负三元组可能不够精确，无法在最近的迭代中进行采样。此外，在阴性样本集中存在假阴性三联体，其中分数也可能很高。因此，我们还需要考虑除缓存中得分最大的三元组之外的其他三元组。这些激励我们在步骤6中使用均匀随机抽样方案。
2. ***Importance sampling strategy to update the cache***: 我们需要在每次迭代中刷新缓存。此外，需要以有效的方式更新缓存。$$p(\bar{h} \mid(t, r))=\frac{\exp (f(\bar{h}, r, t))}{\sum_{h_i \in \hat{\mathcal{H}}_{(r, t)}} \exp \left(f\left(\bar{h}_i, r, t\right)\right)}$$

# 总结
提供了一种负采样策略: 每次随机抽取固定个数M的负样本，与原负样本N组合，取出评分最高的N个作为新的负样本。