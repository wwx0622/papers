# title
[Knowledge Graph Embedding Based Question Answering](http://research.baidu.com/Public/uploads/5c1c9a58317b3.pdf)
《基于KGE的QA》

# Abstract
QA-KG旨在利用KG中的事实回答自然语言问题。人们提出了许多KGE方法。关键思想是将每个谓词/实体表示为一个低维向量，这样可以保留KG中的关系信息。所学习的向量可以有益于各种应用，例如KGC和推荐。

本文探讨如何使用它们来处理QA-KG问题。

然而，这仍然是一项具有挑战性的任务，因为谓词可以在自然语言问题中以不同的方式表达。此外，实体名称和部分名称的模糊性使得可能的答案数量很大。

为了弥补这一差距，本文提出了一种有效的基于知识嵌入的问答框架(Knowledge Embedding based Question Answering, KEQA)。我们专注于回答最常见类型的问题，即简单问题，其中，如果正确识别了单个头部实体和单个谓词，机器可以直接回答每个问题。为了回答一个简单的问题，KEQA的目标不是直接推断其头部实体和谓词，而是在KGE空间中联合恢复问题的头部实体、谓词和尾部实体表示。基于精心设计的关节距离度量，KG中最接近的事实向量作为答案返回。

# Conclusion
KG-QA的挑战:
- 因为谓词可能具有不同的自然语言表达式。机器很难捕获它们的语义信息。
- 即使假设问题的实体名称被正确识别，实体名称和部分名称的模糊性仍然会使候选实体的数量增加。

KEQA：旨在解决简单问题，即QA-KG中最常见的问题类型。KEQA建议在KG嵌入空间中联合恢复问题的头部实体、谓词和尾部实体表示，而不是直接推断头部实体和谓词。
基于注意的双向LSTM模型用于执行谓词和头部实体表示学习。由于与KG中的所有实体进行比较是昂贵和嘈杂的，所以使用**头部实体检测模型**来选择问题中的连续标记作为头部实体的名称，使得候选头部实体集将减少为具有相同或类似名称的实体的数量。
给定预测事实$(\hat{e}_h, \hat{p}_ℓ, \hat{e}_t）$，一个精心设计的联合距离度量用于测量其到所有候选事实的距离。返回具有最小距离的事实作为答案。在大型基准上的实验表明，KEQA比所有最先进的方法实现了更好的性能。

在未来的工作中，我们计划研究后续的开放问题。
- KEQA基于预训练的KGE执行问答。我们如何通过联合进行KG嵌入和问题回答来推进它？
- 现实世界的知识图表和培训问题经常动态更新。我们如何扩展我们的框架来处理这个场景？

# Introduction
KG基础;
KGQA: 将最终用户的自然语言问题自动转换为结构化查询，如SPARQL，并返回KG中的实体和/或谓词作为答案
应用范围: 从搜索引擎设计到会话代理构建。
子问题: 如语义分析和实体链接
基于KGE的QA-KG三个难点:
1. 谓词在自然语言问题中通常有各种表达式。这些表达式可能与谓词名称非常不同。例如谓词*person.nationality*，该为此可以表示为可以表达为"what is ...'s nationality"、“which country is...from”、“where is ... from”等。
2. 即使假设实体名称可以被准确识别，实体名称和部分名称的模糊性仍然会导致难以找到正确的实体，因为候选人的数量往往很大。
   - 问答中可能只涉及实体的部分，如人名、地名等
   - 因候选实体数量大导致很难确定实体
3. 终端用户问题的领域通常是无限的，新问题可能涉及与训练中不同的谓词。这对QA-KG算法的鲁棒性提出了要求。

简单问题是一个自然语言问题，只涉及一个头部实体和一个谓词。
通过分析问题，本文旨在回答三个研究问题。
1. 如何应用谓词嵌入表示来弥合自然语言表达式和KG谓词之间的差距
2. 如何利用实体嵌入表示来应对模糊挑战？
3. 如何利用KGE表示中保存的全文关系来推进QA-KG框架？

本文贡献:
1. 形式化定义了基于KGE的QA困难。
2. 提出了KEQA，它可以通过在知识图嵌入空间中联合恢复头实体、谓词和尾实体表示来回答自然语言问题。
3. 设计一个联合距离度量，将KGE表示中保存的结构和关系考虑在内。
4. 在大型基准（即简单问题）上实证证明了KEQA的有效性和稳健性。

# Problem Statement
操作: $\mathbf{s} = [\mathbf{x;h}]$，结合向量x，h为新向量s

![重要符号](../../image/KEQA%2001.png "一些重要符号")

**Definition1 (Simple Question)**: 如果自然语言问题只涉及知识图中的单个头部实体和单个谓词，并将其尾部实体作为答案，则该问题称为简单问题

**基于KGE的QA**: 给定与所有谓词和实体名称的嵌入表示$P\&E$、关系函数$f(·)$，以及与对应的头实体和谓词相关联的一组简单问题Q，我们的目标是设计一个端到端框架，以一个新的简单问题作为输入，并自动返回相应的头实体和谓词。该框架的性能通过正确预测头部实体和谓词的准确性来评估。

# Knowledge Embedding based on QA-KG
为了准确预测头部实体和谓词，我们提出了基于KGE的问答框架。其主要思想如下图所示。![KEQA框架](../../image/KEQA%2002.png "KEQA框架") 
KG已经嵌入到两个低维空间中，$(h,ℓ,t)$可以表示为三个潜在向量，即$(e_h，p_ℓ, e_t)$。
KEQA通过三个步骤实现目标。
1. 基于Q中的问题及其谓词的嵌入表示，KEQA训练谓词学习模型，该模型将问题作为输入并返回位于KG嵌入空间中的向量$\hat{p}_ℓ$ 作为预测谓词表示。类似地，头实体学习模型训练
2. 由于KG中的实体数量通常很大，KEQA采用头部实体检测模型来减少候选头部实体。主要目标是将问题中的几个标记识别为预测的头部实体名称，然后将搜索空间从整个实体减少到具有相同或相似名称的多个实体。则$\hat{e}_h$主要用于处理模糊性挑战。
3. 给定KG嵌入算法定义的关系函数$f(\cdot)$，KEQA计算预测尾部实体表示$\hat{e}_t$. 基于精心设计的关节距离度量，预测事实$(\hat{e}_h, p_\mathcal{\ell}, \hat{e}_t)$，返回在G中最接近的事实作为问题答案。

## KGE
得到两个嵌入空间$\mathbf{P,\ E}$
## Predicate and Head Entity Learning Models
![谓词与头实体学习模型](../../image/KEQA%2003.png "谓词与头实体学习模型")
采用了一个简单的神经网络架构，如上图所示。它主要由双向递归神经网络层和注意层组成。核心思想是考虑单词的顺序和重要性。不同顺序的单词可能有不同的含义，单词的重要性也可能不同。
### Neural Network Based Predicate Representation Learning
LSTM是RNN的典型示例。给定一个长度为L的问题，我们首先基于预训练模型（如GloVe），将其L个标记映射为一系列字嵌入向量$\{\mathbf{x}_{j}\}$。然后，我们使用BiLSTM学习前向隐藏状态序列$:(\overrightarrow{{{\bf h}}}_{1},\overrightarrow{{{\bf h}}}_{2},\ldots,\overrightarrow{{{\bf h}}}_{L})$和反向隐藏状态序列$:(\overleftarrow{{{\bf h}}}_{1},\overleftarrow{{{\bf h}}}_{2},\ldots,\overleftarrow{{{\bf h}}}_{L})$。之后${\bf h}_{j}=[\overrightarrow{{{\bf h}}}_{j};\,\overleftarrow{{{{\bf h}}}}_{j}].$
注意力层:$$\alpha_{j}=\frac{\exp(q_{j})}{\sum_{i=1}^{L}\exp(q_{i})}$$$$q_{j}=\operatorname{tanh}(\mathbf{w}^{\mathsf{T}}[\mathbf{x}_{j};\mathbf{h}_{j}]+b_{q}).$$
隐藏状态: $s_j=[x_j；α_jh_j]$，通过全连接层得到第j个令牌的目标向量$\mathbf{r}_j\in R^{d\times1}$。预测谓词表示:$\hat{\bf p}_\ell=\frac{1}{L}\sum_{j=1}^{L}\mathbf{r}_{j}^{\mathsf{T}}.$
### Neural Network Head Entity Learning Model
类似上一节的模型得到预测实体表示$\hat{e}_h$
为了减少找到候选头实体的花销，使用下面的模型
## Head Entity Detection Model
![图3](../../image/KEQA%2004.png "减少头实体匹配成本")
在这一步中，我们的目标是选择一个问题中的一个或多个连续标记作为头部实体的名称，以便搜索空间可以从整个实体减少到具有相同或相似名称的多个实体。

该头部实体检测（HED）模型的架构如上图所示。其结构类似于谓词/头部实体学习模型，但没有注意层。前面的步骤类似，后面应用全连接层和softmax函数应用于$h_j$，从而得到目标向量$v_j∈ R^{2×1}$。$v_j$中的两个值对应于第j个令牌属于两个标签类别的概率，即实体名称令牌和非实体名称令牌。通过这种方式，我们对每个标记进行分类，并将一个或多个标记识别为头部实体名称。我们将这些标记表示为$HED_{entity}$，将问题中的其余标记表示为$HED_{non}$。

我们使用Q中的问题及其头部实体名称作为训练数据来训练HED模型。由于这些问题中的实体名称标记是连续的，训练模型也将以高概率返回连续标记作为$HED_{entity}$。如果返回是离散的，则每个连续部分将被视为独立的头部实体名称。应注意的是，HED实体可能只是正确头部实体名称的一部分。
因此，包含或与$HED_{entity}$相同的所有实体将被包括为候选头部实体，这可能仍然很大，因为许多实体将在一个大KG中共享相同的名称。

## Joint Search on Embedding Spaces

### Joint Distance Metric

联合距离度量: $$\operatorname{minimize}_{(h, \ell, t) \in C} \left\|\mathbf{p}_{\ell}-\hat{\mathbf{p}}_{\ell}\right\|_{2}+\beta_{1}\left\|\mathbf{e}_{h}-\hat{\mathbf{e}}_{h}\right\|_{2}+\beta_{2}\left\|f\left(\mathbf{e}_{h}, \mathbf{p}_{\ell}\right)-\hat{\mathbf{e}}_{t}\right\|_{2}-\beta_{3} \operatorname{sim}\left[n(h), \mathrm{HED}_{\text {entity}}\right]-\beta_{4} \operatorname{sim}\left[n(\ell), \mathrm{HED}_{\mathrm{non}}\right]$$
$sim[\cdot,\cdot]$表示相似度函数,戴帽子的通过神经网络预测的谓词、实体，不戴帽子的候选向量。
上式前三项测量事实$(h，ℓ, t)$以及我们在KG嵌入空间中的预测之间的距离。我们使用$f(e_h，p_ℓ)$表示尾部实体的嵌入向量，而不是$e_t$。这是因为在KG中，可能有几个事实具有相同的头部实体和谓词，但不同的尾部实体。因此，单个尾部实体$e_t$可能无法回答该问题。
同时，$f(e_h，p_ℓ)$匹配预测的尾部实体$\hat{e}_t$，因为它也是基于f（·）推断的。我们倾向于选择一个事实，其头部实体名称与$HED_{entity}$完全相同，并且谓词名称与问题中提到的相同。上式中的第四项和第五项实现这两个目标。返回最小化目标函数的事实$(h^*, \ell^*, t^*)$。
### Knowledge Embedding based Question Answering
算法:
![算法](../../image/KEQA%2005.png "KEQA 算法")

# 总结
- KGE部分没有新意。
- Q的头实体与谓词提取模型基于神经网络实现
- HED模型用于降低匹配复杂度，?可用于LP,KGC

***
可借鉴部分不多