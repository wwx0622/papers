# title
[SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions](https://arxiv.org/abs/2212.10560)

# Abstact

大型 **“指令调整(instruction-tuned)”** 语言模型（即，微调以响应指令）已经证明了将零样本推广到新任务的显著能力。然而，它们在很大程度上依赖于人类书面指令数据，而这些数据在数量、多样性和创造性方面往往是有限的。

我们介绍了**self-instruction**，这是一个框架，用于通过引导自己的一代来提高预训练语言模型的指令跟随能力。我们的管道从语言模型中生成指令、输入和输出样本，然后过滤无效或类似的样本，然后使用它们来微调原始模型。

# Instruction

目前NLP领域在构建遵循自然语言指令的模型方面的关键在于两部分：**大型预训练语言模型(LM)** 以及 **人类书面指令数据(提示(prompt source)和超自然指令(super natural instructions))**。但是收集指令数据往往昂贵且多样性有限，无法覆盖真实任务的分布以及描述方式。

![self-instruction 流程图](../../image/self-instruction%2001.PNG "self-instruction 流程图")
*图1*

**self-instruction**，是一个半自动化的过程，用于使用来自模型本身的指令信号来调整预训练的LM。整个过程是一个迭代自举算法（见图1），它从一个有限的手动编写的任务种子集开始，用于指导整个生成。
1. 系统会提示模型为新任务生成指令。此步骤利用现有的指令集来创建更广泛的指令，这些指令定义（通常是新的）任务。
2. 给定新生成的指令集，框架还为它们创建输入输出实例，这些实例稍后可用于监督指令调优。
3. 在将剩余的有效任务添加到任务池之前，使用各种启发式方法来自动过滤低质量或重复的指令。

这个过程可以重复多次迭代，直到达到大量的任务

本文贡献是：
1. 引入了**self-instruction**，这是一种用最少的人类标记数据诱导指令跟随能力的方法；
2. 通过大量的教学调整实验证明了它的有效性；
3. 发布了一个52K指令的大型合成数据集和一组手工编写的新任务，用于构建和评估未来的指令跟随模型。

# Method

流程见图1。

## Difining Instruction Data

要生成的指令数据包含一组指令$\{𝐼_𝑡\}$, 每个指令用自然语言定义了一个任务$𝑡$。任务$𝑡$有$𝑛_𝑡 ≥ 1$个输入输出实例$\{(𝑋_{𝑡,𝑖}, 𝑌_{𝑡,𝑖})\}^{𝑛_𝑡}_{𝑖=1}$.模型$M$被期望在给定任务指令和相应输入的情况下产生输出：$𝑀(𝐼_𝑡 , 𝑋_{𝑡,𝑖}) = 𝑌_{𝑡,𝑖}$其中$𝑖 \in \{1，…，𝑛_𝑡 \}$. 请注意，在许多情况下，指令和实例输入没有严格的边界。例如，“写一篇关于学校安全的文章”可以是一个有效的指令，我们希望模型直接回应，同时它也可以被表述为“写一份关于以下主题的文章”作为指令，“学校安全”作为实例输入。为了鼓励数据格式的多样性，我们允许这种不需要额外输入的指令（即$𝑋$为空）。

笔者注: 假如一个指令为"写一个关于以下主题的文章", 任务$t$包含若干个实例，如$\{X=学校安全，Y_{学校安全}\}$、$\{X=教育，Y_{教育}\}$......, $M$使得$M(I=写一个关于以下主题的文章, X=学校安全)=Y_{学校安全}$......

### Automatic Instruction Data Generation

我们的数据生成流水线由四个步骤组成：
1. 生成任务指令
2. 确定指令是否代表分类任务
3. 使用输入优先或输出优先的方法生成实例
4. 过滤低质量数据。

***1. Instruction Generation***: Self-Instruct以自举方式从一小组种子人工编写的指令中生成新的指令。该文用175个任务启动任务池（每个任务有1条指令和1个实例）。对于每个步骤，从该池中抽取8(可以理解为一个batch)条任务指令作为上下文示例。在8条指令中，6条来自人工编写的任务，2条来自之前步骤中模型生成的任务，以促进多样性。提示模板如下图所示。

![指令自举生成](../../image/self-instruct%2002.png)

***2. Classification Task Identification***: 因为需要两种不同的方法来处理分类任务和非分类任务，所以接下来需要确定生成的指令是否代表分类任务。我们使用来自种子任务的12条分类指令和19条非分类指令，以几种方式提示LM确定这一点。提示模板如下图所示。

![确定指令是否为分类任务](../../image/self-instruct%2003.png)

***3. Instance Generation***: 给定指令及其任务类型，为每个指令独立生成实例。这是具有挑战性的，因为它要求模型根据指令理解目标任务是什么，找出需要哪些额外的输入字段并生成它们，最后通过生成输出来完成任务。我们发现，当使用来自其他任务的上下文示例提示指令输入输出时，预训练的LMs可以在很大程度上实现这一点。
一种自然的方法是Input-first Approach，在这种方法中，我们可以要求LM首先根据指令提出输入字段，然后生成相应的输出。这种生成顺序类似于模型用来响应指令和输入的方式，但这里有来自其他任务的上下文示例。提示模板下图所示。
![Input-first Template](../../image/self-instruct%2005.png)
然而，我们发现这种方法可以生成偏向于一个标签的输入，特别是对于分类任务(例如，对于语法错误检测，它通常会生成语法输入)。因此，我们为分类任务额外提出了一种输出优先的方法，在这种方法中，我们首先生成可能的类标签，然后在每个类标签上约束输入生成。提示模板如表8所示。我们将输出优先方法应用于前一步确定的分类任务，并将输入优先方法应用于剩余的非分类任务。
![Output-first Template](../../image/self-instruct%2005.png)
**笔者注**:接受指令$I_t$后，LM需要根据生成$\{X_{t,i}, Y_{t, i}\}$。一种自然的想法是输入优先，也即先生成$X_t$后再生成$Y_t$，但是在分类任务中该方法往往将生成一个标签作为输入，例如情感分类的，可能生成积极、消极作为输入。面对这类任务(分类任务)设计了输出优先方法，首先生成可能的类标签，在根据指令和类标签生成一段文本。
<font color="red">总之:
- 分类方法(输出优先): $Instruction \rightarrow Class Label(Output) \rightarrow Input Text$
- 非分类方法(输入优先): $Instruction \rightarrow Input Text \rightarrow Output Text$
  
其中$X_t=Input, Y_t=Output$</font>

***4. Filtering and Postprocessing***: 为了鼓励多样性，只有当新指令与任何现有指令的ROUGE-L相似性小于0.7时，才会将其添加到任务池中。我们还排除了包含一些特定关键字（例如，图像、图片、图表）的指令，这些关键字通常不能由LM处理。当为每条指令生成新实例时，我们过滤掉完全相同的实例或具有相同输入但不同输出的实例。根据启发式方法识别并过滤无效的生成（例如，指令太长或太短，实例输出是输入的重复）。

<font color="red">知识点: </font>Rouge-L使用最长公共子序列衡量。$$R_{lcs}=\frac{LCS(X,Y)}{m}$$$$R_{lcs}=\frac{LCS(X,Y)}{n}$$$$F_{lcs}=\frac{(1+\beta^2)R_{lcs}P_{lcs}}{R_{lcs} + \beta^2P_{lcs}}$$，其中$LCS(X,Y)$表示X,Y的最长公共子序列, m,n分别表示参考摘要和自动摘要的长度。$R_{lcs}$, $P_{lcs}$分别表示召回率与准确率，最后的$F_{lcs}$即为Rouge-L。

## Finetuning the LM to Follow Instructions
在创建大规模指令数据后，我们使用它来微调原始LM（即SELF-instruction）。为此，我们将指令和实例输入连接起来作为提示，并训练模型以标准监督的方式生成实例输出。为了使模型对不同的格式具有鲁棒性，我们使用多个模板将指令和实例输入编码在一起。例如，指令可以以“任务：”为前缀，输入可以以“输入：”为后缀，提示结尾可以附加“输出：”，中间可以放不同数量的换行符，等等。

# Self-instruct Data from GPT3
在本节中，我们将我们的方法引入GPT3作为案例研究。我们使用通过OpenAI API访问的最大GPT3LM（“davinci”引擎）。
## Statistics
下表表示生成的数据，其中有超过52k个指令和超过82k个与制定对应的实例
![统计](../../image/self-instruct%2006.png)
## Diversity
为了研究生成的指令类型及其多样性，我们确定了生成指令中的动名词结构。我们使用Berkeley Neural Parser7来解析指令，然后提取最接近词根的动词及其第一个直接名词宾语。52445条指令中有26559条包含这样的结构；其他说明通常包含更复杂的条款（例如，“分类这条推文是否包含政治内容。”）或被框定为问题（例如：“这些陈述中哪些是真的？”）。我们在下图中绘制了前20个最常见的词根动词及其前4个直接名词宾语，它们占整个集合的14%。总的来说，我们在这些说明中看到了相当多样化的意图和文本格式。
![词根分布](../../image/self-instruct%2007.png)

此外，在指令与种子指令的Rouge-L分数、指令长度、输入长度、输出长度等不同方面都有多样性的体现。

# Experimental Results
与其他baselines比较在各种指令调整设置下的性能。
## $GPT3_{self-inst}$: finetuning GPT3 on its own instruction data

给定指令生成的指令数据，我们使用GPT3模型本身（“davinci”引擎）进行指令调优。如§2.3所述，我们使用各种模板来连接指令和输入，并训练模型以生成输出。
这个微调是通过OpenAI微调API完成的。8我们使用默认的超参数，除了我们将提示损失权重设置为0，并为2个时期训练模型。我们请读者参阅附录A.3，了解更多微调细节。
所得到的模型由GPT3SELF-INST表示。

# baselines
- 现成的LMs。我们评估了T5-LM和GPT3作为普通LM baselines（仅预训练，没有额外微调）。这些基线将表明现成的LMs在预训练后能够立即自然地遵循指令的程度。
- 公开提供的指令调优模型。T0和T𝑘-instruction是两个指令调优模型，分别被证明能够在许多NLP任务中遵循指令。这两个模型都是从T5检查点进行微调的，并且是公开的。对于这两个型号，我们都使用了它们的最大版本，参数为11B。
- 指令调优的GPT3模型。我们评估了InstructGPT，它是由OpenAI基于GPT3开发的，以更好地遵循人类指令，并被社区发现具有令人印象深刻的零样本能力。这些模型有不同的版本，其中较新的版本使用更广泛的数据或算法新颖性。对于我们在§4.3中的SUPERNI实验，我们只与他们的text-davinci-001引擎进行比较，因为他们的较新引擎是用最新的用户数据训练的，并且可能已经看过SUPERNI测试集。为了对新编写的说明书进行人工评估，我们包括了它们的001、002和003引擎以确保完整性。
- 此外，为了将SELF-instruction训练与其他公开可用的指令调整数据进行比较，我们使用PROMPSTOURCE和SUPERNI的数据进一步微调GPT3模型，这些数据用于训练T0和T𝑘-说明型号。我们分别简称它们为T0训练和SUPERNI训练。为了节省训练预算，我们为每个数据集采样了5万个实例（但涵盖了它们的所有指令），其大小与我们生成的指令数据相当。基于王等人的发现和我们的早期实验，减少每个训练任务的实例数量不会降低模型对看不见任务的泛化性能。

## Experiment 1: Zero-Shot Generalization on SUPERNI benchmark

![SUPERNI 实验结果](../../image/self-instruct%2008.png)

## Experiment 2: Generalization to User-oriented Instructions on Novel Tasks
面相用户的应用程序驱动的新指令，我们首先对大型LMs可能有用的各个领域进行头脑风暴（例如，电子邮件写作、社交媒体、生产力工具、娱乐、编程），然后制定与每个领域相关的指令以及输入输出实例（同样，输入是可选的）。目标是为了任务风格和格式多样化，最终创建了252条指令。
有人工进行评估。有四个评级：
- A: 有效且令人满意
- B: 可接受但有瑕疵
- C: 有相关性但是内容大量错误，例如第一句回复相关，后面开始瞎编乱造
- D: 响应不相关或完全无效

![生成面向用户的指令](../../image/self-instruct%2009.png)

## Effect of Data Size and Quality

***Data Size***: 数据大小。
SELF-instruction提供了一种在几乎没有人为标记的情况下以低成本增长指令数据的方法；生成的数据越多，指令跟随能力会更好吗？我们通过从生成的数据集中对不同数量的指令进行二次采样，对采样的子集进行微调GPT3，并评估生成的模型在252个面向用户的指令集上的表现，来分析生成的数据的大小。我们进行与§4.4相同的人类评估。随着数据量提升模型持续改进，直到16k条时收敛。
有趣的是，在对SUPERNI进行评估时，我们发现该模型的性能增益早些时候在数百条指令左右处于平稳状态。这可能是由于新生成的数据被丢弃

***Data quality***: 数据质量。
提高模型性能的另一个方向是获取我们生成的数据，并获得更好的监督（减少噪音）。我们通过使用InstructGPT003（可用的最佳通用模型）来重新生成给定指令和输入的所有实例的输出字段来探索这个想法。然后，我们使用数据的改进版本来微调GPT3。这可以被视为我们的数据对InstructionGPT003的升华。生成的模型比用原始数据训练的模型高出10%，这表明未来在使用我们的生成管道获取初始数据，然后与人类专家一起提高数据质量或从更好的模型中提取数据方面有很大的空间。

# Conclusion

我们介绍了SELF-instruction，这是一种通过LMs自己生成指令数据来提高其指令跟随能力的方法。在对普通GPT3进行实验时，我们为不同的任务自动构建了一个由52K指令组成的大规模数据集，并在此数据上微调GPT3，使SUPERNI比原始GPT3有33%的绝对改进。此外，我们还为新颖的任务策划了一套专家书面说明。
对该集的人工评估表明，使用SELF-instruction调优GPT3的性能大大优于使用现有公共指令数据集，并且与InstructionGPT001的性能非常接近。我们希望“自我指导”可以作为调整预训练的LMs以遵循人类指令的第一步，未来的工作可以建立在这些数据的基础上，以改进指令遵循模型。

# Limitations

***Tail phenomena***: 依赖于LM，同样受LM的缺点限制，而尾部现象造成了挑战。也即对于低频环境的收益较小

***Dependence on large models***: 依赖于LM的归纳偏差，所以适用于大模型。

***Reinforcing LM biases***: 可能放大有问题的数据，例如社会偏见，包括性别、种族等。