# paper

[SELFDEFEND: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner](https://arxiv.org/abs/2406.05498)

## 方案

通过prompt直接检测(P_direct)或通过思维链间接检测(P_indent)输入中的有害内容，使模型安全机制生效

![alt text](image.png)
![alt text](image-1.png)